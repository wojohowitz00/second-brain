---
phase: 03-ollama-connection
plan: 01
type: tdd
wave: 1
depends_on: ["02-vault-scanner"]
files_modified:
  - backend/_scripts/ollama_client.py
  - backend/tests/test_ollama_client.py
  - backend/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "App can detect if Ollama server is running (http://localhost:11434)"
    - "App can check if specified model (llama3.2:3b) is available"
    - "App can send chat messages and receive responses"
    - "App handles connection errors gracefully (server not running)"
    - "App handles timeout errors gracefully (model loading)"
    - "App handles model not found errors gracefully (404)"
  artifacts:
    - path: "backend/_scripts/ollama_client.py"
      provides: "OllamaClient class with health check and chat methods"
      min_lines: 120
      exports: ["OllamaClient", "HealthStatus", "OllamaError", "OllamaServerNotRunning", "OllamaModelNotFound", "OllamaTimeout"]
    - path: "backend/tests/test_ollama_client.py"
      provides: "Unit tests for Ollama client"
      min_lines: 100
  key_links:
    - from: "backend/_scripts/ollama_client.py"
      to: "ollama"
      via: "import"
      pattern: "from ollama import Client"
---

<objective>
Create a robust Ollama client module with health checks, error handling, and chat/generate capabilities. This is a CRITICAL CHECKPOINT phase - local LLM functionality must be validated before investing in UI.

Purpose: Enable local LLM classification via Ollama for thought capture routing.

Output: `ollama_client.py` module that can detect Ollama status, verify model availability, and execute chat operations with comprehensive error handling.
</objective>

<execution_context>
@/Users/richardyu/.claude/get-shit-done/workflows/execute-plan.md
@/Users/richardyu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ollama-connection/03-RESEARCH.md
@backend/_scripts/slack_client.py
@backend/tests/conftest.py
</context>

<feature>
  <name>Ollama Client Module</name>
  <files>
    backend/_scripts/ollama_client.py
    backend/tests/test_ollama_client.py
  </files>
  <behavior>
    OllamaClient class with following capabilities:

    1. Health Check:
       - is_server_running() -> bool
       - is_model_available() -> bool
       - health_check() -> HealthStatus dataclass
       - list_models() -> list[str]

    2. LLM Operations:
       - chat(messages: list[dict]) -> dict
       - generate(prompt: str) -> dict
       - verify_model_responds() -> bool

    3. Error Handling:
       - OllamaServerNotRunning - connection refused
       - OllamaModelNotFound - 404 response
       - OllamaTimeout - request timed out
       - OllamaError - base exception

    Test cases (TDD - write tests FIRST):

    Health Check Tests:
    1. test_server_not_running_returns_false - ConnectError → False
    2. test_server_running_returns_true - Success → True
    3. test_model_available_exact_match - Model in list → True
    4. test_model_available_prefix_match - Model prefix matches → True
    5. test_model_not_available_returns_false - Model not in list → False
    6. test_health_check_all_ok - Server + model OK → ready=True
    7. test_health_check_server_down - No server → error message
    8. test_health_check_model_missing - No model → error message
    9. test_list_models_returns_names - Returns list of model names

    Chat/Generate Tests:
    10. test_chat_returns_response - Mock success → response dict
    11. test_chat_server_not_running_raises - ConnectError → OllamaServerNotRunning
    12. test_chat_timeout_raises - TimeoutException → OllamaTimeout
    13. test_chat_model_not_found_raises - 404 → OllamaModelNotFound
    14. test_generate_returns_response - Mock success → response dict
    15. test_verify_model_responds_success - Response received → True
    16. test_verify_model_responds_failure - Error → False

    Integration Tests (marked, skip if no Ollama):
    17. test_real_health_check - Against real server
    18. test_real_chat_response - Real chat operation
  </behavior>
  <implementation>
    Create ollama_client.py following 03-RESEARCH.md patterns:

    1. Configuration:
       - DEFAULT_HOST = "http://localhost:11434"
       - DEFAULT_MODEL = "llama3.2:3b"
       - DEFAULT_TIMEOUT = 30.0 (generous for cold start)
       - HEALTH_CHECK_TIMEOUT = 5.0 (quick checks)

    2. HealthStatus dataclass:
       - server_running: bool
       - model_available: bool
       - model_name: str
       - ready: bool
       - error: Optional[str]

    3. Custom exceptions:
       - OllamaError (base)
       - OllamaServerNotRunning
       - OllamaModelNotFound
       - OllamaTimeout

    4. OllamaClient class:
       - __init__(host, model, timeout)
       - Lazy client initialization
       - Separate health_client with shorter timeout
       - All methods with proper error handling

    Use patterns from 03-RESEARCH.md code examples.
    Follow slack_client.py patterns for exception structure.
  </implementation>
</feature>

<feature>
  <name>Dependency Addition</name>
  <files>
    backend/pyproject.toml
  </files>
  <behavior>
    Add ollama dependency to project:
    
    ```toml
    dependencies = [
        ...existing deps...
        "ollama>=0.4.0",
    ]
    ```
    
    Then run: `uv sync` to install
  </behavior>
</feature>

<verification>
Add dependency and run tests:
```bash
cd /Users/richardyu/PARA/Personal/1_Projects/apps/second-brain/backend
uv add ollama
uv run pytest tests/test_ollama_client.py -v --tb=short
```

Integration test (requires Ollama running with llama3.2:3b):
```bash
# First ensure Ollama is running and model is pulled:
# ollama serve (if not using Ollama.app)
# ollama pull llama3.2:3b

uv run pytest tests/test_ollama_client.py -v -m integration
```

Manual verification:
```bash
cd backend/_scripts
python3 -c "
from ollama_client import OllamaClient
client = OllamaClient()
status = client.health_check()
print(f'Ready: {status.ready}')
if status.error:
    print(f'Error: {status.error}')
else:
    response = client.chat([{'role': 'user', 'content': 'Say hello'}])
    print(f'Response: {response.message.content}')
"
```
</verification>

<success_criteria>
1. Tests written FIRST, fail initially (RED phase)
2. is_server_running() correctly detects Ollama server status
3. is_model_available() correctly checks model availability
4. health_check() returns comprehensive HealthStatus
5. chat() sends messages and returns responses
6. All three error types raised appropriately:
   - OllamaServerNotRunning on connection failure
   - OllamaModelNotFound on 404
   - OllamaTimeout on timeout
7. At least 16 unit tests covering all scenarios
8. Integration tests pass when Ollama is running
9. `uv run pytest tests/test_ollama_client.py -v` exits with code 0
</success_criteria>

<output>
After completion, create `.planning/phases/03-ollama-connection/03-01-SUMMARY.md`
</output>
